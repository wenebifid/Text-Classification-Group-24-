{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04275d6c",
   "metadata": {},
   "source": [
    "# Traditional ML Text Classification\n",
    "## Logistic Regression vs SVM with Multiple Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ac237",
   "metadata": {},
   "source": [
    "### 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaced8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e65d8",
   "metadata": {},
   "source": [
    "### 2. Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"/mnt/data/labeled_data.csv\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf51da",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39263241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "sns.countplot(x='label', data=data)\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "data['text_length'] = data['text'].apply(lambda x: len(str(x).split()))\n",
    "plt.figure()\n",
    "sns.histplot(data['text_length'], bins=40)\n",
    "plt.title(\"Text Length Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16325b",
   "metadata": {},
   "source": [
    "### 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    return [t for t in tokens if t not in stop_words]\n",
    "\n",
    "data['tokens'] = data['text'].apply(preprocess)\n",
    "data['clean_text'] = data['tokens'].apply(lambda x: \" \".join(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39cfd6",
   "metadata": {},
   "source": [
    "### 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2163f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['clean_text'], data['label'],\n",
    "    test_size=0.2, random_state=42, stratify=data['label']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45463e2",
   "metadata": {},
   "source": [
    "### 6. TF-IDF Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44cd864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07256ac8",
   "metadata": {},
   "source": [
    "### 7. Logistic Regression vs SVM (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Linear SVM\": LinearSVC()\n",
    "}\n",
    "\n",
    "results_tfidf = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    preds = model.predict(X_test_tfidf)\n",
    "    results_tfidf.append({\n",
    "        \"Model\": name,\n",
    "        \"Embedding\": \"TF-IDF\",\n",
    "        \"Accuracy\": accuracy_score(y_test, preds),\n",
    "        \"F1\": f1_score(y_test, preds, average='weighted')\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb9c10",
   "metadata": {},
   "source": [
    "### 8. Word2Vec Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74716578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w2v = Word2Vec(sentences=data['tokens'], vector_size=100, window=5, min_count=2)\n",
    "\n",
    "def sentence_vector(tokens, model):\n",
    "    vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    return np.mean(vecs, axis=0) if len(vecs) > 0 else np.zeros(model.vector_size)\n",
    "\n",
    "X_w2v = np.array([sentence_vector(t, w2v) for t in data['tokens']])\n",
    "\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
    "    X_w2v, data['label'], test_size=0.2, random_state=42, stratify=data['label']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15732c3",
   "metadata": {},
   "source": [
    "### 9. Logistic Regression vs SVM (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_w2v = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_w2v, y_train_w2v)\n",
    "    preds = model.predict(X_test_w2v)\n",
    "    results_w2v.append({\n",
    "        \"Model\": name,\n",
    "        \"Embedding\": \"Word2Vec\",\n",
    "        \"Accuracy\": accuracy_score(y_test_w2v, preds),\n",
    "        \"F1\": f1_score(y_test_w2v, preds, average='weighted')\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results_w2v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624888c3",
   "metadata": {},
   "source": [
    "### 10. Combined Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = pd.DataFrame(results_tfidf + results_w2v)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519f271a",
   "metadata": {},
   "source": [
    "### 11. Visualization: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7336015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "sns.barplot(data=results_df, x=\"Embedding\", y=\"F1\", hue=\"Model\")\n",
    "plt.title(\"Traditional ML Models Comparison\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}